# Notes of **Computer Architecture: A Quantitative Approach**


|时间|内容|
|:---|:---|
|2022-04-05| kick off. |
|2022-04-12| add: Appendix A. Instruction Set Principles. |

## 术语

<!-- 记录阅读过程中出现的关键字及其简单的解释. -->

## 介绍

<!-- 描述书籍阐述观点的来源、拟解决的关键性问题和采用的方法论等. -->

## 动机

<!-- 描述阅读书籍的动机, 要达到什么目的等. -->

## 概念结构

<!-- 描述书籍的行文结构, 核心主题和子主题的内容结构和关系. -->

### 1 Fundamentals of Quantitative Design and Analysis
#### 1.1 Introduction
#### 1.2 Classes of Computers
#### 1.3 Defining Computer Architecture
#### 1.4 Trends in Technology
#### 1.5 Trends in Power and Energy in Integrated Circuits
#### 1.6 Trends in Cost
#### 1.7 Dependability
#### 1.8 Measuring, Reporting, and Summarizing Performance
#### 1.9 Quantitative Principles of Computer Design
#### 1.10 Putting It All Together: Performance, Price, and Power
#### 1.11 Fallacies and Pitfalls
#### 1.12 Concluding Remarks
#### 1.13 Historical Perspectives and References

### 2 Memory Hierarchy Design
#### 2.1 Introduction
#### 2.2 Memory Technology and Optimizations
#### 2.3 Ten Advanced Optimizations of Cache Performance
#### 2.4 Virtual Memory and Virtual Machines
#### 2.5 Cross-Cutting Issues: The Design of Memory Hierarchies
#### 2.6 Putting It All Together: Memory Hierarchies in the ARM Cortex-A53 and Intel Core i7 6700
#### 2.7 Fallacies and Pitfalls
#### 2.8 Concluding Remarks: Looking Ahead
#### 2.9 Historical Perspectives and References

### 3 Instruction-Level Parallelism and Its Exploitation
#### 3.1 Instruction-Level Parallelism: Concepts and Challenges
#### 3.2 Basic Compiler Techniques for Exposing ILP
#### 3.3 Reducing Branch Costs With Advanced Branch Prediction
#### 3.4 Overcoming Data Hazards With Dynamic Scheduling
#### 3.5 Dynamic Scheduling: Examples and the Algorithm
#### 3.6 Hardware-Based Speculation
#### 3.7 Exploiting ILP Using Multiple Issue and Static Scheduling
#### 3.8 Exploiting ILP Using Dynamic Scheduling, Multiple Issue, and Speculation
#### 3.9 Advanced Techniques for Instruction Delivery and Speculation
#### 3.10 Cross-Cutting Issues
#### 3.11 Multithreading: Exploiting Thread-Level Parallelism to Improve Uniprocessor Throughput
#### 3.12 Putting It All Together: The Intel Core i7 6700 and ARM Cortex-A53
#### 3.13 Fallacies and Pitfalls
#### 3.14 Concluding Remarks: What’s Ahead?
#### 3.15 Historical Perspective and References

### 4 Data-Level Parallelism in Vector, SIMD, and GPU Architectures
#### 4.1 Introduction
#### 4.2 Vector Architecture
#### 4.3 SIMD Instruction Set Extensions for Multimedia
#### 4.4 Graphics Processing Units
#### 4.5 Detecting and Enhancing Loop-Level Parallelism
#### 4.6 Cross-Cutting Issues
#### 4.7 Putting It All Together: Embedded Versus Server GPUs and Tesla Versus Core i7
#### 4.8 Fallacies and Pitfalls
#### 4.9 Concluding Remarks
#### 4.10 Historical Perspective and References

### 5 Thread-Level Parallelism
#### 5.1 Introduction
#### 5.2 Centralized Shared-Memory Architectures
#### 5.3 Performance of Symmetric Shared-Memory Multiprocessors
#### 5.4 Distributed Shared-Memory and Directory-Based Coherence
#### 5.5 Synchronization: The Basics
#### 5.6 Models of Memory Consistency: An Introduction
#### 5.7 Cross-Cutting Issues
#### 5.8 Putting It All Together: Multicore Processors and Their Performance
#### 5.9 Fallacies and Pitfalls
#### 5.10 The Future of Multicore Scaling
#### 5.11 Concluding Remarks
#### 5.12 Historical Perspectives and References

### 6 Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism
#### 6.1 Introduction
#### 6.2 Programming Models and Workloads for Warehouse-Scale Computers
#### 6.3 Computer Architecture of Warehouse-Scale Computers
#### 6.4 The Efficiency and Cost of Warehouse-Scale Computers
#### 6.5 Cloud Computing: The Return of Utility Computing
#### 6.6 Cross-Cutting Issues
#### 6.7 Putting It All Together: A Google Warehouse-Scale Computer
#### 6.8 Fallacies and Pitfalls
#### 6.9 Concluding Remarks
#### 6.10 Historical Perspectives and References

### 7 Domain-Specific Architectures
#### 7.1 Introduction
#### 7.2 Guidelines for DSAs
#### 7.3 Example Domain: Deep Neural Networks
#### 7.4 Google’s Tensor Processing Unit, an Inference Data Center Accelerator
#### 7.5 Microsoft Catapult, a Flexible Data Center Accelerator
#### 7.6 Intel Crest, a Data Center Accelerator for Training
#### 7.7 Pixel Visual Core, a Personal Mobile Device Image Processing Unit
#### 7.8 Cross-Cutting Issues
#### 7.9 Putting It All Together: CPUs Versus GPUs Versus DNN Accelerators
#### 7.10 Fallacies and Pitfalls
#### 7.11 Concluding Remarks
#### 7.12 Historical Perspectives and References

## 总结

<!-- 概要记录书籍中如何解决关键性问题的. -->

## 应用

<!-- 记录如何使用书籍中方法论解决你自己的问题. -->

## 文献引用

<!-- 记录相关的和进一步阅读资料: 文献、网页链接等. -->

- John L. Hennessy, David A. Patterson. **Computer Architecture: A Quantitative Approach, 6th Edition**. Morgan Kaufmann: 2017.
- **The RISC-V Instruction Set Manual Volume I: Unprivileged ISA**. Document Version 20191213.

## 其他备注
